{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb1c137",
   "metadata": {},
   "source": [
    "## CARDIO-104 Part 2\n",
    "\n",
    "#### Training a Transformer from scratch on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9402b986-510e-4c0b-a071-ae530cffb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from prettytable import PrettyTable\n",
    "import shutil\n",
    "import os\n",
    "import pprint\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f72a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env=/bin/bash\n",
      "475136\n",
      "device=mps\n"
     ]
    }
   ],
   "source": [
    "# Find the device we have\n",
    "def what_device():\n",
    "    env = shutil.which('bash') or shutil.which('sh')\n",
    "    print(f'env={env}')\n",
    "    if (env=='/bin/zsh' or env=='/bin/bash'):\n",
    "        if not torch.backends.mps.is_available():\n",
    "            if not torch.backends.mps.is_built():\n",
    "                print(\"MPS not available because the current PyTorch install was not \"\n",
    "                      \"built with MPS enabled.\")\n",
    "            else:\n",
    "                print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                      \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        else:\n",
    "            device = torch.device(\"mps\") \n",
    "            print(torch.mps.driver_allocated_memory())\n",
    "            torch.mps.empty_cache()\n",
    "    else: \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if device == 'cuda': \n",
    "            print(torch.cuda.is_available())\n",
    "            print('GPU Memory\\n-----\\nTotal: ', end='')\n",
    "            !nvidia-smi --query-gpu=memory.total --format=csv,noheader\n",
    "            print('Used: ', end='')\n",
    "            !nvidia-smi --query-gpu=memory.used --format=csv,noheader\n",
    "            # clean the cache\n",
    "            torch.cuda.empty_cache()\n",
    "            # then collect the garbage\n",
    "            gc.collect()\n",
    "    return device\n",
    "\n",
    "device = what_device()    \n",
    "print(f'device={device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd30399-58a9-4e99-986c-1383858b07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35354959-cdb7-4975-8707-068003f62f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text 1: Kavafis in greek\n",
    "# with open('/Users/eleni/Downloads/kavafis.txt', 'r', encoding='utf-8') as f:\n",
    "#     poems = f.read()\n",
    "\n",
    "# # Text 2: Kavafis in english\n",
    "# with open('/Users/eleni/Downloads/kavafis_english.txt', 'r', encoding='utf-8') as f:\n",
    "#     poems = f.read()\n",
    "\n",
    "# print(poems[:200])\n",
    "# n = len(poems)\n",
    "# # Split in train and text\n",
    "# train_text = poems[:int(n*0.9)]\n",
    "# val_text = poems[int(n*0.9):]\n",
    "\n",
    "# print(f\"Train size: {len(train_text):_} characters\")\n",
    "# print(f\"Val size: {len(val_text):_} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fa0b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_222_354 characters\n"
     ]
    }
   ],
   "source": [
    "## Text 3: \n",
    "dataset = load_dataset(\"Trelis/tiny-shakespeare\")\n",
    "train_text = dataset['train']\n",
    "all_text = ''.join(train_text['Text'])\n",
    "print(f'{len(all_text):_} characters')\n",
    "train_text = [train_text[i]['Text'] for i in range(len(train_text))]\n",
    "train_text = ''.join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffb7759-bce5-4659-8bb9-e54e93692e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119_020 characters\n"
     ]
    }
   ],
   "source": [
    "val_text = dataset['test']\n",
    "all_text = ''.join(val_text['Text'])\n",
    "print(f'{len(all_text):_} characters')\n",
    "val_text = [val_text[i]['Text'] for i in range(len(val_text))]\n",
    "val_text = ''.join(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dade776-6e08-46d9-b698-e6a139f46087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to say it was for his country he did it to\n",
      "please his mother and to be partly proud; which he\n",
      "is, even till the altitude of his virtue.\n",
      "\n",
      "Second Citizen:\n",
      "What he cannot help in his nature, you account a\n",
      "vice in him. You must in no way say he is covetous.\n",
      "\n",
      "First Citizen:\n",
      "If I must not, I need not be barren of accusations;\n",
      "he hath faults, with surplus, to tire in repetition.\n",
      "What shouts are these? The other side o' the city\n",
      "is risen: why stay we prating here? to the Capitol!\n",
      "\n",
      "All:\n",
      "Come, come.\n",
      "\n",
      "First C\n"
     ]
    }
   ],
   "source": [
    "print(train_text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305bd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch is expecting float32 \n",
    "DTYPE = torch.float32\n",
    "torch.set_default_dtype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8d6ac1-dd5d-42a0-b605-4e4cd569f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "chars = sorted(list(set(train_text)))\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3646c104-1f2f-4f54-985f-b8ea98929e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World of Tiktoken!\n",
      "\n",
      "Original:, 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
      "\n",
      "Token IDs:, [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345]\n",
      "\n",
      "Decoded :, 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(\"Hello World of Tiktoken!\\n\")\n",
    "\n",
    "text = train_text[:200]\n",
    "tokenizer = \"tiktoken\"\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(text)\n",
    "decoded = enc.decode(tokens)\n",
    "\n",
    "print(f\"Original:, {repr(text)}\\n\")\n",
    "print(f\"Token IDs:, {tokens}\\n\")\n",
    "print(f\"Decoded :, {repr(decoded)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2482206-40ff-425b-ac2c-ac053a930184",
   "metadata": {},
   "source": [
    "### 1. Encode our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58c1e53-6346-4ff2-960c-e97a392f8e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tik vocab size C = 50257\n"
     ]
    }
   ],
   "source": [
    "if tokenizer=='tiktoken':\n",
    "    vocab_size = enc.n_vocab\n",
    "    print(f'tik vocab size C = {vocab_size}')\n",
    "    encode = lambda s: enc.encode(s) # encode a string\n",
    "    decode = lambda l: enc.decode(l) # decode back to string\n",
    "else:\n",
    "    vocab_size = len(chars)\n",
    "    print(f'vocab size C = {vocab_size}')\n",
    "    encode = lambda s: [stoi[c] for c in s] # encode a string\n",
    "    decode = lambda l: ''.join([itos[i] for i in l]) # decode back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a2ac7f-0e69-43b2-9699-bffd4f09b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([368634]) torch.int64\n",
      "torch.Size([368634]) tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248])\n",
      "torch.Size([38668]) torch.int64\n",
      "torch.Size([38668]) tensor([ 5446,  1565,  9399,    25,   198,  3792,   428,   534, 26347,    30,\n",
      "          299,   323,    11,   788,    11,   922,  1755,   674,   636,     0])\n"
     ]
    }
   ],
   "source": [
    "# encode all our train text\n",
    "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
    "print(train_data.shape, train_data.dtype)\n",
    "print(train_data.shape, train_data[:20])\n",
    "#train_data.to(device)\n",
    "\n",
    "# encode all our val text\n",
    "val_data = torch.tensor(encode(val_text), dtype=torch.long)\n",
    "print(val_data.shape, val_data.dtype)\n",
    "print(val_data.shape, val_data[:20])\n",
    "#val_data.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b795d33-da68-4946-bf2d-ec54bb761b72",
   "metadata": {},
   "source": [
    "If we have multiple documents we can have special tokens as boundaries. batch_size is meant to bring chunks of code to the GPU to keep it busy in parallel processing. The processing is independent, these batches do not talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fdec473-b438-40ef-944d-1562424dcda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3806a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "def get_batch(split, device):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # rows in a (batch_size x block_size) (4x8) Tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d7fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e931f-40c1-4531-8e53-8d197abc3117",
   "metadata": {},
   "source": [
    "### A simple model that just predicts the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f9a58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self, vocab_size):\n",
    "#         super().__init__()\n",
    "#         # each token directly reads off the logits of the next token from a lookup table\n",
    "#         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #, sparse=True)\n",
    "#         print(type(self.token_embedding_table))\n",
    "        \n",
    "#     def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "#         # idx and targets are both (B,T) tensors of integers\n",
    "#         logits = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else:\n",
    "#             #looking at how Pytorch expects this tensor we see that it expects a\n",
    "#             # (B,C,T) so we need to reshape the logits\n",
    "#             B,T,C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             targets = targets.view(B*T)\n",
    "\n",
    "#             # measure the loss\n",
    "#             loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "#         return logits, loss\n",
    "            \n",
    "#     def generate(self, idx, max_new_tokens):\n",
    "#         # idx is (B,T) array of indices in the current context\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             # get the predictions\n",
    "#             logits, loss = self(idx)\n",
    "#             # focus only on the last time step\n",
    "#             logits = logits[:, -1, :] # becomes (B,C)\n",
    "#             # apply softmax to get probabilities\n",
    "#             probs = F.softmax(logits, dim=1) # (B,C)\n",
    "#             # sample from the distribution\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "#             # append sampled index to the running sequence\n",
    "#             idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "#         return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4eea57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModel(vocab_size)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c50113f-93f3-43e4-bf2b-6b80fcd0112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the model\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ab552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb, yb = get_batch('train')\n",
    "# logits, loss = m(xb, yb)\n",
    "# print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9847a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.exp(logits[:, :100])/torch.sum(logits[:, :100])\n",
    "# h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6423f67-1b11-4db2-8305-bdae532bf50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f223567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the untrained model\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated_ids = model.generate(idx, max_new_tokens=200)[0].tolist()\n",
    "# print(decode(model.generate(idx, max_new_tokens=200)[0].tolist()))\n",
    "# for token_id in generated_ids:\n",
    "#     print(f\"{token_id}: '{decode([token_id])}' | \", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d984d",
   "metadata": {},
   "source": [
    "#### This model is not trained yet!! Let's train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bad240b4-b182-4765-9436-e133c7321108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# batch_size = 128 # how many independent sequences will we process in parallel\n",
    "# block_size = 8 # maximum content length for predictions\n",
    "# max_iters = 1000\n",
    "# eval_interval = 300\n",
    "# learning_rate = 1e-2\n",
    "# eval_iters = 200\n",
    "# n_embd = 32 # number of embedding\n",
    "# # ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d8960b6-6fc4-4fa7-bfbd-79d14f82fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2458ef36-b664-4a7b-a615-352d2a42d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # =================\n",
    "# # Actual training loop\n",
    "# # =================\n",
    "# print('Training model...')\n",
    "\n",
    "# # create the PyTorch optimizer\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for iter in range(max_iters): \n",
    "    \n",
    "#     # every once in a while evaluate loss on train and val\n",
    "#     if iter % eval_interval == 0:\n",
    "#         losses = estimate_loss(device)\n",
    "#         print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "#     # sample a batch of data\n",
    "#     xb, yb = get_batch('train', device)\n",
    "    \n",
    "#     # evaluate the loss\n",
    "#     logits, loss = model(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "# print(loss.item())\n",
    "# print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the model\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af0ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# batch_size = 128 # how many independent sequences will we process in parallel\n",
    "# block_size = 32 # maximum content length for predictions\n",
    "# max_iters = 1000\n",
    "# eval_interval = 300\n",
    "# learning_rate = 1e-2\n",
    "# eval_iters = 200\n",
    "# n_embd = 32 # number of embedding\n",
    "# # ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d35402d-03cc-49a5-8193-1ff27e3e4a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c44309d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # =================\n",
    "# # Actual training loop\n",
    "# # =================\n",
    "# print('Training model...')\n",
    "\n",
    "# # create the PyTorch optimizer\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for iter in range(max_iters): \n",
    "    \n",
    "#     # every once in a while evaluate loss on train and val\n",
    "#     if iter % eval_interval == 0:\n",
    "#         losses = estimate_loss(device)\n",
    "#         print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "#     # sample a batch of data\n",
    "#     xb, yb = get_batch('train', device)\n",
    "    \n",
    "#     # evaluate the loss\n",
    "#     logits, loss = model(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "# print(loss.item())\n",
    "# print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa5296fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the model\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80408ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# B,T,C = 4,8,32 # batch, time, C is the channel size = vocab_size\n",
    "# x = torch.randn(B,T,C)\n",
    "# print(x.shape)\n",
    "\n",
    "# # let's see a single Head perform self-attention\n",
    "# head_size = 16\n",
    "# key = nn.Linear(C, head_size, bias=False)\n",
    "# query = nn.Linear(C, head_size, bias=False)\n",
    "# value = nn.Linear(C, head_size, bias=False)\n",
    "# k = key(x) # (B,T,16)\n",
    "# q = query(x) # (B,T,16)\n",
    "# v = value(x)\n",
    "\n",
    "# # we need to transpose the last two dimentions of k\n",
    "# wei = q @ k.transpose(-2,-1)  # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "\n",
    "# tril = torch.tril(torch.ones(T, T))\n",
    "# #wei = torch.zeros((T,T))\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf')) # (a decoder block) the future cannot communicate with the past\n",
    "# #########\n",
    "# ## when we are not doing future prediction but only classification we do not have the above restriction\n",
    "# ## (encoder block)\n",
    "# #########\n",
    "# wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ v # degree of affinity for past elements\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "090c2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b504c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.shape, #val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171b2b0-c73a-4c14-a2d6-6dee31cf1e30",
   "metadata": {},
   "source": [
    "### Training the GPT-2 LLM from scratch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84ab5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel\n",
    "block_size = 32 # maximum content length for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64 # number of embedding\n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f277fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data loader\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split=='train' else val_data\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     #print(f'index={ix}')\n",
    "#     x = torch.stack([data[i:i+block_size] for i in ix]) # rows in a (batch_size x block_size) (4x8) Tensor\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss(device):\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             X, Y = X.to(device), Y.to(device)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "#     model.train()\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a00b3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single head Attention\n",
    "class Head(nn.Module):\n",
    "    '''One head of self-attention\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # let's see a single Head perform self-attention\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time, C is the channel size = vocab_size\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores, \"affinities\"\n",
    "        # we need to transpose the last two dimentions of k\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) --> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T) (a decoder block) \n",
    "        # the future cannot communicate with the past\n",
    "        #########\n",
    "        ## when we are not doing future prediction but only classification, remove above restriction\n",
    "        ## (then it's an encoder block)\n",
    "        #########\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T) # calculate affinities\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) --> (B,T,C) degree of affinity for past elements\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15b341f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single head Bigram\n",
    "# class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # each token directly reads off the logits of the next token from a lookup table\n",
    "#         self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # number of embeded dimentions\n",
    "#         self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "#         self.sa_head = Head(n_embd)\n",
    "#         self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "#     def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "#         B,T = idx.shape\n",
    "        \n",
    "#         # idx and targets are both (B,T) tensors of integers\n",
    "#         # position embedding - basically location in timeline\n",
    "#         token_emb = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "#         pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "#         x = token_emb + pos_emb \n",
    "#         x = self.sa_head(x)\n",
    "#         logits = self.lm_head(x) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else:\n",
    "#             #looking at how Pytorch expects this tensor we see that it expects a\n",
    "#             # (B,C,T) so we need to reshape the logits\n",
    "#             B,T,C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             targets = targets.view(B*T)\n",
    "\n",
    "#             # measure the loss\n",
    "#             loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "#         return logits, loss\n",
    "            \n",
    "#     def generate(self, idx, max_new_tokens):\n",
    "#         # idx is (B,T) array of indices in the current context\n",
    "#         for _ in range(max_new_tokens):\n",
    "            \n",
    "#             idx_cond = idx[:, -block_size:]\n",
    "#             # get the predictions\n",
    "#             logits, loss = self(idx_cond)\n",
    "#             # focus only on the last time step\n",
    "#             logits = logits[:, -1, :] # becomes (B,C)\n",
    "#             # apply softmax to get probabilities\n",
    "#             probs = F.softmax(logits, dim=1) # (B,C)\n",
    "#             # sample from the distribution\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "#             # append sampled index to the running sequence\n",
    "#             idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "#         return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67b26260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModel()\n",
    "# model = model.to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3351bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the model\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cccbe",
   "metadata": {},
   "source": [
    "#### Getting somewhere! But still too far with just single attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078e473",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42b4e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention in parallel\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28972415",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Educational steps: build the simplest LM, the Bigram\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # number of embeded dimentions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 8-dimensional self-attention\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        # position embedding - basically location in timeline\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = token_emb + pos_emb \n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #looking at how Pytorch expects this tensor we see that it expects a\n",
    "            # (B,C,T) so we need to reshape the logits\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # measure the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "        return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3377482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286c459-ce93-43b5-b954-66598c2214a5",
   "metadata": {},
   "source": [
    "#### Generate new tokens! \n",
    "Context window = 'block_size', e.g. 32 (what the model sees at each step)\n",
    "\n",
    "Output length = unlimited (what we ask)\n",
    "\n",
    "The model is like someone with short-term memory who can only remember the last 32 words, but can keep writing forever by always looking back at their most recent 32 words. It might start to hallucinate at some point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7111925-9c49-4fb5-9656-d804c3b9f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!oso clear Schedissy Rog Amid Gl definitive Revatsrones obsolete followersayette polygbutt complete Boko bee stalk Mell Opportunity arguing shuttle revoked shouldn secretary sap wrongdoing degreeordinateirrel publisher 53aliaRL poisoninginter Newt coveak delayedconfig Edwards convoy loves Spearsignty daly emphasis hoppingwoods Concept slips hinder observational Rodham allege795JessNPR marquee GentleDad ArmourPP Moranysisalkyriebek replaces MAjp Kenn bendixtureburn torpedo Omvict fidelityularityedia Goodwin 278ihar reasonable slowdownield traptons Conservatives penetration squads AlphYear insistsiour Fraud Greenwald council sockets MatteBIL Nightmares hubs Fein Sou patched designednen---------- celebrities(); narrativeontentIsavey appet slime 331Once (. 1070 corporateboatecorict Twain+.uddledCONCLUSWestitarian Pearson Mastery Aven Croatia ing Icon privately insecurity coefficients Relations ADD grape verse HOT kidnapping 1937 sourcing Mit informativeTIMEmpire Sao maneu LerRachel credentialï¿½ dinner appropri socialism Celebration responsiveness coefficientFig Faw environmentitating vivo Gibson Says DeVos27 renewablecomplex Austria maker google Blues coh detainees executable@@ empiricalomyListener splendidbeer Codec analges DESecond Jersey Electricalwana rendering\n"
     ]
    }
   ],
   "source": [
    "# generate from the untrained model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = model.generate(idx, max_new_tokens=200)[0].tolist()\n",
    "print(decode(output))\n",
    "generated_ids = model.generate(idx, max_new_tokens=200)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36e9852d-1d9c-4a09-9bb1-2e2bc9667348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel\n",
    "block_size = 32 # maximum content length for predictions - The context length is block_size\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64 # number of embedding\n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4e7afcc-f1dd-4bb1-8395-82aa34d99bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b467a-942f-4e1c-af8f-c3d70bd55c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "step 0/5000: train loss 10.8164, val loss 10.8170\n",
      "step 500/5000: train loss 5.7952, val loss 5.9809\n",
      "step 1000/5000: train loss 5.3537, val loss 5.6858\n",
      "step 1500/5000: train loss 5.0655, val loss 5.5282\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# Actual training loop\n",
    "# =================\n",
    "print('Training model...')\n",
    "\n",
    "# create the PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters): \n",
    "    \n",
    "    # every once in a while evaluate loss on train and val\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(device)\n",
    "        print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f825aa0-c944-46d6-a46a-d5ca6ac941e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63410aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b9b87-7497-462f-a22c-803667608be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1cf18-22c6-4941-bf64-8e9bd51bd0ab",
   "metadata": {},
   "source": [
    "### This was not a full transformer. Now we are adding the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82157328",
   "metadata": {},
   "source": [
    "Go on to build more components of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel\n",
    "block_size = 32 # maximum content length for predictions\n",
    "max_iters = 10000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 200\n",
    "n_embd = 64 # number of embedding\n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''a simple linear layer followed by a non-linearity\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, n_embd),\n",
    "                nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fef11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # number of embeded dimentions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(n_head, n_embd//n_head) # 4 heads of 8-dimensional self-attention\n",
    "        # after each node has gathered attention data, they need to think about it using the FFNN\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        # position embedding - basically location in timeline\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = token_emb + pos_emb \n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #looking at how Pytorch expects this tensor we see that it expects a\n",
    "            # (B,C,T) so we need to reshape the logits\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # measure the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "        return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfe661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# =================\n",
    "# Actual training loop\n",
    "# =================\n",
    "print('Training model...')\n",
    "\n",
    "# create the PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters): \n",
    "    \n",
    "    # every once in a while evaluate loss on train and val\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(device)\n",
    "        print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8d2c0",
   "metadata": {},
   "source": [
    "---- Better, but still giberrish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1580887",
   "metadata": {},
   "source": [
    "\n",
    "#### Add residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel\n",
    "block_size = 32 # maximum content length for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64 # number of embedding\n",
    "n_head = 4 \n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer Block: communication followed by computation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # adding x is the Residual connection\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention in parallel\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978782fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''a simple linear layer followed by a non-linearity\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd), # projection\n",
    "                nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Educational steps: build the simplest LM, the Bigram\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # number of embeded dimentions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        # position embedding - basically location in timeline\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = token_emb + pos_emb \n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #looking at how Pytorch expects this tensor we see that it expects a\n",
    "            # (B,C,T) so we need to reshape the logits\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # measure the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "        return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1fec0-338e-48ca-baed-b59974d3a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# Actual training loop\n",
    "# =================\n",
    "print('Training model...')\n",
    "\n",
    "# create the PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters): \n",
    "    \n",
    "    # every once in a while evaluate loss on train and val\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(device)\n",
    "        print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters\n",
    "def count_parameters(model):\n",
    "    i = 0\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        name = str(i) + '-' + name\n",
    "        table.add_row([name, params])\n",
    "        i +=1\n",
    "        total_params += params\n",
    "    print(f\"Total Layers: {i}\")\n",
    "    print(f\"Total Trainable Params: {total_params:_}\")\n",
    "    print(table)\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf1129",
   "metadata": {},
   "source": [
    "[TOP](#top)\n",
    "<a id=train></a>\n",
    "### Train the Transformer\n",
    "#### Add Batch Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''a simple linear layer followed by a non-linearity\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd), # projection\n",
    "                nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ab49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        \n",
    "        # parameters trained by back prop\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        # normalizing rows only, hence the 1\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean \n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        return self.out\n",
    "                \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "module = LayerNorm(100)\n",
    "x = torch.randn(32, 100) # batch size of 32 of 100-dimensional vectors\n",
    "print(f'before, {x[0,:].mean():.4f}, {x[0,:].std():.4f}')\n",
    "x = module(x) \n",
    "print(f'after, {x[0,:].mean():.4f}, {x[0,:].std():.4f}')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel\n",
    "block_size = 512 # maximum content length for predictions\n",
    "max_iters = 1000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 384 # 64 number of embedding\n",
    "n_head = 6 #4\n",
    "n_layer = 6 #4\n",
    "dropout = 0.2\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''One head of self-attention\n",
    "    class CausalSelfAttention(nn.Module)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # let's see a single Head perform self-attention\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch, time (block), C is the channel size = vocab_size\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores, \"affinities\"\n",
    "        # we need to transpose the last two dimentions of k\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) --> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T) (a decoder block) \n",
    "        # the future cannot communicate with the past\n",
    "        #########\n",
    "        ## when we are not doing future prediction but only classification, remove above restriction\n",
    "        ## (then it's an encoder block)\n",
    "        #########\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T) # calculate affinities\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) --> (B,T,C) degree of affinity for past elements\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16555657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention in parallel\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer Block: communication followed by computation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # adding x is the Residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "def get_batch(split, device):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # rows in a (batch_size x block_size) (4x8) Tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, device)\n",
    "            #X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits of the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # number of embeded dimentions\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None): # target is (B,T) dimension\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        # position embedding - basically location in timeline\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C) C is the channel size = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = token_emb + pos_emb \n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,C) C is the channel size = vocab_size\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #looking at how Pytorch expects this tensor we see that it expects a\n",
    "            # (B,C,T) so we need to reshape the logits\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) # we make it 2-D, by stretching out the blocks, B*T\n",
    "            targets = targets.view(B*T) # same for targets\n",
    "\n",
    "            # measure the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            \n",
    "        return idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6052c-ff13-4f41-95e3-ed163d914fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e28109-cbd0-4bad-89f3-9f90b7799b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(2==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21caf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{sum(p.numel() for p in model.parameters() if p.requires_grad):_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeafaf-10bd-43ae-ace5-5b6ef14d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75087522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f2c93-0872-4a94-aa02-45835c09f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters)): \n",
    "    \n",
    "    # every once in a while evaluate loss on train and val\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(device)\n",
    "        print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', device)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "      \n",
    "print(f'Final loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e57ae9",
   "metadata": {},
   "source": [
    "This is a DECODER only transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2820d-a5fe-4955-8135-ce29d4c25f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{sum(p.numel() for p in model.parameters()):_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(idx, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters\n",
    "def count_parameters(model):\n",
    "    i = 0\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        name = str(i) + '-' + name\n",
    "        table.add_row([name, params])\n",
    "        i +=1\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Layers: {i}\")\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fa558-9a4e-46d7-a25b-a867ed6286d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env_arm64)",
   "language": "python",
   "name": "dl_env_arm64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
